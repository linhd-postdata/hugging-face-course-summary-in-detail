{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A full training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this notebook is in the Hugging Face course, chapter 3, section 4: [A full training](https://huggingface.co/course/chapter3/4?fw=pt)\n",
    "\n",
    "The original code of this notebook is in the Hugging Face's SageMaker repository: [section4_pt.ipynb](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been tested in the following environment:\n",
    "- Environment: Project created in [Paperspace Gradient](https://gradient.paperspace.com) with Python 3.9.13.\n",
    "- Machine: P5000 (30GiB RAM 8 CPU 16GiB GPU) (more details on [Paperspace Machines](https://docs.paperspace.com/gradient/machines/)).\n",
    "- IDE: Visual Studio Code using remote Jupyter server."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the libraries datasets v2.7.1, evaluate v0.3.0, and transformers v4.25.1 with quiet and upgrade flags.\n",
    "%pip install -q datasets==2.7.1 evaluate==0.3.0 transformers==4.25.1 --upgrade\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# %pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets, tokenizer and data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b0464d285472caf1132d74ddd2977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9e65bde4507e139d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be742090b4f408698fd19a12f339f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-be6d3eb930bf62fd.arrow\n"
     ]
    }
   ],
   "source": [
    "# Import load_dataset from the Datasets library.\n",
    "from datasets import load_dataset\n",
    "# Import AutoTokenizer and DataCollatorWithPadding from the Transformers library.\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# Load the raw_dataset with the name mrpc from the Datasets library.\n",
    "raw_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "# Create a checkpoint with the name bert-base-cased.\n",
    "checkpoint = \"bert-base-cased\"\n",
    "# Create a tokenizer with the AutoTokenizer class and the checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Create a function to tokenize the examples.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Tokenize the raw_dataset with the tokenize_function.\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "# Create a DataCollatorWithPadding with the tokenizer.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Remove the columns sentence1, sentence2 and idx from the tokenized_dataset.\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Rename the column label to labels.\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "# Set the format of the columns to torch.\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "# Print the columns name of the train tokenized_dataset.\n",
    "print(tokenized_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataLoaders from the PyTorch library.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a train_dataloader with the train tokenized_dataset and the data_collator.\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "# Create a validation_dataloader with the validation tokenized_dataset and the data_collator.\n",
    "validation_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70])}\n"
     ]
    }
   ],
   "source": [
    "# Inspect only a batch from the train_dataloader with a for loop.\n",
    "for batch in train_dataloader:\n",
    "    # Create a dictionary with the keys and the shape of the batch items.\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import AutoModelForSequenceClassification from the Transformers library.\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Create a model with the AutoModelForSequenceClassification class and the checkpoint with the num_labels argument set to 2.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6831, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Pass the batch to the model.\n",
    "outputs = model(**batch)\n",
    "# Print the loss and the shape of the logits.\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import AdamW from the Transformers library.\n",
    "from transformers import AdamW\n",
    "\n",
    "# Create an optimizer with the AdamW class and the model parameters.\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "# Import get_scheduler from the Transformers library.\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Set the number of epochs to 3.\n",
    "num_epochs = 3\n",
    "# Set the total number of training steps to the number of batches in the train_dataloader multiplied by the number of epochs.\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "# Create a learning rate scheduler with the get_scheduler function, the optimizer, the number of training steps and the number of epochs.\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "# Print the total number of training steps.\n",
    "print(total_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch library.\n",
    "import torch\n",
    "\n",
    "# Set the device to cuda if available.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Move the model to the device.\n",
    "model.to(device)\n",
    "# Print the device with the text \"Device type:\".\n",
    "print(\"Device type:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92892bedc66345289356113c11cc2ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import tqdm library for using in Visual Studio Code.\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create a progress bar with the range of the total number of training steps.\n",
    "progress_bar = tqdm(range(total_steps))\n",
    "\n",
    "# Train the model with the train method.\n",
    "model.train()\n",
    "# Create a for loop with the number of epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    # Create a for loop with the train_dataloader.\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move the batch to the device.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Pass the batch to the model.\n",
    "        outputs = model(**batch)\n",
    "        # Get the loss from the outputs.\n",
    "        loss = outputs.loss\n",
    "        # Zero the gradients.\n",
    "        model.zero_grad()\n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "        # Update the parameters.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        lr_scheduler.step()\n",
    "        # Update the progress bar.\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8333333333333334, 'f1': 0.8839590443686006}\n"
     ]
    }
   ],
   "source": [
    "# Import the Evaluate library.\n",
    "import evaluate\n",
    "\n",
    "# Create metrics with the load method from the Evaluate library.\n",
    "metrics = evaluate.load(\"glue\", \"mrpc\")\n",
    "# Evaluate the model with the eval method.\n",
    "model.eval()\n",
    "# Create a for loop with the validation_dataloader.\n",
    "for batch in validation_dataloader:\n",
    "    # Move the batch to the device.\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # Pass the batch to the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    # Get the predictions from the outputs.\n",
    "    predictions = outputs.logits.argmax(-1)\n",
    "    # Update the metrics with the predictions and the labels.\n",
    "    metrics.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# Compute the metrics with the compute method.\n",
    "metrics = metrics.compute()\n",
    "# Print the metrics.\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
